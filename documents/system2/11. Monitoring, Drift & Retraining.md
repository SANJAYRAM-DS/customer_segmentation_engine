# Monitoring, Drift & Retraining

## Purpose

This document defines **how System 2 is continuously monitored, how drift is detected, and how retraining decisions are made**.

The goals are to:
- Detect degradation early
- Prevent silent performance decay
- Avoid unnecessary retraining
- Maintain long-term trust in predictions

Monitoring is designed to be **actionable, not noisy**.

---

## 1. Data Drift Monitoring

### 1.1 Objective

Data drift monitoring ensures that **incoming data remains compatible with training assumptions**.

The system monitors data health continuously but treats drift as a **diagnostic signal**, not an automatic retraining trigger.

---

### 1.2 Monitored Input Signals

The following feature groups are actively monitored:

#### Demand Signals
- Sales volume distributions
- Zero-demand frequency
- Demand volatility metrics

#### Promotion Signals
- Promotion frequency
- Promotion depth and duration
- Promotion overlap patterns

#### Supply Signals
- Lead time distributions
- Supplier reliability indicators
- Inventory buffer levels

#### Temporal Signals
- Calendar feature distributions
- Seasonality phase alignment
- Holiday encoding consistency

Each signal is monitored per **SKU segment**, not globally.

---

### 1.3 Drift Detection Methods

Multiple drift detectors are used to reduce false positives.

#### Distribution-Based Metrics
- Population Stability Index (PSI)
- Kolmogorov–Smirnov (KS) distance
- Jensen–Shannon (JS) divergence

#### Data Quality Metrics
- Missing value rate shifts
- Out-of-range value frequency
- Cardinality changes (new SKUs, stores)

#### Schema & Timestamp Checks
- Column presence and type validation
- Timestamp monotonicity
- Timezone consistency

All drift metrics are tracked as **time series**, not point checks.

---

### 1.4 Alerting Strategy

Alerts are tiered by severity.

#### Soft Alerts
- Early drift signals
- Gradual distribution shifts
- Minor missing data increases

Soft alerts trigger investigation but **no immediate action**.

#### Hard Alerts
- Schema violations
- Severe timestamp issues
- Data feed outages

Hard alerts block downstream pipelines until resolved.

---

### 1.5 Drift Response Policy

Data drift:
- Does NOT automatically trigger retraining
- Requires human review
- Is correlated with performance metrics before action

This prevents **overfitting to noise**.

---

## 2. Model Drift Monitoring

### 2.1 Objective

Model drift monitoring focuses on **prediction behavior and decision quality**, not just input features.

The system distinguishes between:
- Temporary fluctuations
- Structural degradation

---

### 2.2 Pre-Label Drift Signals

Before ground truth is available, the system monitors **behavioral proxies**.

#### Prediction Behavior Metrics
- Forecast uncertainty inflation
- Prediction variance over time
- Sudden shifts in output distributions

#### Stability Indicators
- Day-over-day forecast deltas
- Prediction smoothness
- Cross-model disagreement (if applicable)

Pre-label signals enable **early warnings**.

---

### 2.3 Post-Label Performance Metrics

Once actual outcomes are available, true performance is evaluated.

#### Forecast Accuracy Metrics
- Rolling MAE
- Rolling WAPE
- Horizon-specific errors

#### Risk Model Metrics
- Stock-out miss rate
- False negative rate
- Cost-weighted error

Metrics are computed per:
- SKU segment
- Store group
- Demand regime

---

### 2.4 Segment-Level Monitoring

Global averages are insufficient.

The system explicitly monitors:

- High-risk SKUs
- Promotional periods
- Long lead-time products
- New or recently onboarded SKUs

Segment failures can trigger actions even if global metrics look healthy.

---

### 2.5 Alert Severity Levels

Model drift alerts are tiered by **business impact**:

- Informational: monitor closely
- Warning: investigate and prepare mitigation
- Critical: initiate rollback or retraining

Alert thresholds are reviewed regularly.

---

## 3. Retraining Triggers

### 3.1 Retraining Philosophy

Retraining is:
- Intentional
- Logged
- Justified

The system avoids **blind retraining loops**.

---

### 3.2 Scheduled Retraining

Scheduled retraining occurs on predictable cadences.

#### Examples
- Weekly retraining for high-velocity SKUs
- Monthly retraining for stable demand
- Pre-season retraining before major cycles

Scheduled retraining ensures baseline freshness.

---

### 3.3 Event-Based Retraining

Event-based retraining is triggered by confirmed signals.

#### Triggers
- Sustained metric degradation
- Confirmed data drift affecting performance
- Major demand regime change
- Supply chain disruptions

Event-based retraining requires approval.

---

### 3.4 Manual Overrides

Manual retraining may be triggered by:

- Business stakeholder requests
- Emergency recalibration needs
- Strategic model updates

Manual overrides are logged and reviewed post-action.

---

### 3.5 Retraining Audit Trail

Every retraining event records:

- Trigger type (scheduled, event-based, manual)
- Affected models and segments
- Training window and data snapshot
- Performance comparison vs previous model

This enables:
- Post-mortems
- Continuous improvement
- Governance audits

---

## 4. Design Guarantees Summary

This monitoring and retraining strategy guarantees:

- Early detection of degradation
- Controlled retraining decisions
- Segment-aware monitoring
- Full auditability

Models are allowed to evolve — **but never silently**.

---

## End of Document