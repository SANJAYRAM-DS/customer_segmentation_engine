# Inference & Serving Strategy

## Purpose

This document defines **how predictions are generated, served, versioned, and consumed** in System 2 (Demand & Inventory Intelligence).

The goal is to ensure:
- Operational robustness
- Safe consumption by downstream systems
- Full traceability and auditability
- Zero silent behavior changes

This system follows a **batch-first, real-time-optional** serving philosophy.

---

## 1. Batch Inference

### 1.1 Role of Batch Inference

Batch inference is the **primary and authoritative prediction path**.

All operational planning, reporting, and automated decision support must rely on batch outputs unless explicitly stated otherwise.

Batch inference prioritizes:
- Deterministic execution
- Cost efficiency
- Predictability
- Easier debugging and rollback

---

### 1.2 Batch Inference Schedule

Batch inference runs on **fixed, predictable cadences**.

#### Forecast Cadences
- **Daily runs**
  - Short-term demand (7–30 day horizon)
  - Used for replenishment and near-term planning
- **Weekly runs**
  - Mid-term demand (30–90 day horizon)
  - Used for inventory positioning and procurement

#### Trigger Conditions
Batch inference is triggered when:
- A new model version is promoted
- Feature pipelines complete successfully
- A scheduled inference window is reached

No inference runs on partially available data.

---

### 1.3 Batch Inference Execution Flow

Each batch run executes the following steps in order:

1. Load **feature snapshots** (time-consistent)
2. Generate **base forecasts** per SKU–store–horizon
3. Apply **hierarchical reconciliation**
4. Compute **stock-out risk scores**
5. Attach **explanations and metadata**
6. Write outputs to the prediction store

All steps are **atomic per run**.

Partial outputs are never exposed.

---

### 1.4 Prediction Granularity

Predictions are generated at the following minimum granularity:

- SKU
- Store (or fulfillment node)
- Forecast horizon (t+1 … t+N)
- Prediction timestamp

This granularity ensures:
- Accurate aggregation
- Safe rollups
- Clear ownership of errors

---

### 1.5 Prediction Storage

All batch predictions are written to a **versioned prediction store**.

#### Storage Characteristics
- Append-only
- Immutable once written
- Partitioned for efficient access

#### Partition Keys
- `prediction_date`
- `model_version`
- `forecast_horizon`

Predictions are **never overwritten**.

---

### 1.6 Batch Consumers

Batch outputs are consumed by:

- Inventory planning systems
- Replenishment optimization pipelines
- BI and analytics dashboards
- Post-hoc evaluation and audits

Consumers must **explicitly select a model version**.

No implicit “latest” access is allowed in production workflows.

---

## 2. Real-Time Inference

### 2.1 Role of Real-Time Inference

Real-time inference is **supplementary**, not primary.

It exists to support:
- Urgent decision checks
- Exploratory simulations
- Exception handling

It does **not replace batch inference**.

---

### 2.2 Approved Use Cases

Real-time inference is allowed only for:

- On-demand stock-out risk checks
- Emergency replenishment decisions
- What-if scenario simulations

Any new use case requires explicit approval.

---

### 2.3 Latency & Reliability Targets

#### Latency
- Soft SLA: hundreds of milliseconds
- No hard real-time guarantees

#### Reliability Philosophy
- Graceful degradation preferred
- Serving stale predictions is acceptable
- Silent failure is not acceptable

---

### 2.4 Fallback Behavior

If real-time inference fails:

1. Serve **last successful batch prediction**
2. Fall back to **rule-based heuristics**
3. Enforce **hard safety thresholds**

Fallback paths are deterministic and tested.

Real-time endpoints never block operational systems.

---

### 2.5 Logic Parity Guarantee

Real-time inference:
- Uses the same feature definitions as batch
- Uses the same trained models
- Introduces **no new business logic**

All real-time logic must be **validated in batch first**.

---

## 3. Output Versioning & Traceability

### 3.1 Predictions as Data Assets

Predictions are treated as **first-class data products**, not ephemeral outputs.

Each prediction is immutable and traceable.

---

### 3.2 Versioning Dimensions

Each prediction is versioned along these axes:

- Model version
- Feature version
- Training run ID
- Prediction timestamp
- Forecast horizon

Versioning enables:
- Safe comparisons
- Reproducibility
- Rollback without recomputation

---

### 3.3 Lineage & Traceability

Every prediction links back to:

- Training data snapshot
- Feature store version
- Model artifact
- Evaluation report

This enables:
- Post-mortems
- Compliance audits
- Root cause analysis

---

### 3.4 Safety Guarantees

The system enforces the following guarantees:

- Predictions are append-only
- No silent overwrites
- Explicit promotion and rollback
- Historical outputs remain accessible

Any change to outputs is **visible and auditable**.

---

## 4. Design Principles Summary

This inference and serving strategy ensures:

- Batch-first robustness
- Safe real-time augmentation
- Deterministic behavior
- Full traceability
- Operational trust

The system favors **boring reliability over clever shortcuts**.

---

## 5. Non-Goals

This serving layer does NOT:
- Perform optimization decisions
- Automatically execute inventory actions
- Hide model versioning from consumers
- Serve unvalidated experimental models

Those concerns belong to downstream systems.

---

## End of Document