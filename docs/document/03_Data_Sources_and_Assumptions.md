# Data Sources & Quality

---

## 3.1 Data Sources

This system aggregates customer behavior from **multiple upstream sources**.
Each source has distinct semantics, update frequencies, and failure modes.

All data sources are treated as **append-only event logs unless stated otherwise**.

---

### 3.1.1 Orders Data

#### Description
Transactional records representing completed customer purchases.

#### Typical Fields
- `order_id`
- `customer_id`
- `order_timestamp`
- `order_amount`
- `currency`
- `order_status`
- `payment_method`
- `channel` (web, app, offline)

#### Usage
- Revenue calculations
- Purchase frequency
- Monetary value features
- CLV target construction

#### Update Pattern
- Near real-time or daily batch
- Late-arriving records possible (payment reconciliation)

---

### 3.1.2 Sessions / Events Data

#### Description
Customer interaction data capturing browsing and engagement behavior.

#### Typical Fields
- `session_id`
- `customer_id` (may be anonymous initially)
- `event_timestamp`
- `event_type` (view, click, add-to-cart)
- `device_type`
- `platform`
- `referrer`

#### Usage
- Engagement intensity
- Recency metrics
- Behavioral trends
- Early churn signals

#### Update Pattern
- Streaming or micro-batch
- Higher volume, noisier data

---

### 3.1.3 Returns Data

#### Description
Records of returned or refunded orders.

#### Typical Fields
- `return_id`
- `order_id`
- `customer_id`
- `return_timestamp`
- `return_reason`
- `refund_amount`

#### Usage
- Net revenue calculation
- Satisfaction proxies
- Negative experience signals
- Churn risk amplification

#### Update Pattern
- Delayed relative to orders
- Often updated asynchronously

---

### 3.1.4 Customer Metadata

#### Description
Relatively static or slowly changing attributes of customers.

#### Typical Fields
- `customer_id`
- `signup_date`
- `acquisition_channel`
- `geography`
- `account_type`
- `loyalty_tier`

#### Usage
- Customer lifecycle context
- Cold-start handling
- Segment enrichment

#### Update Pattern
- Slowly changing dimensions (SCD)
- Infrequent updates

---

### 3.1.5 Derived Feature Tables (Internal)

#### Description
Pre-aggregated, time-windowed features generated by feature pipelines.

#### Examples
- RFM metrics
- Engagement trends
- Return rates
- Velocity features

#### Usage
- Model-ready inputs
- Consistent offline/online features

---

## 3.2 Historical Coverage

Understanding **how far back reliable data exists** is critical for:
- Label construction
- Feature windows
- Backtesting validity

---

### 3.2.1 Orders History

- Earliest reliable data: **[e.g., 3–5 years back]**
- Known gaps:
  - Early system migrations
  - Partial data during peak outages
- Seasonality coverage:
  - Multiple annual cycles available

---

### 3.2.2 Sessions History

- Typically shorter retention than orders
- Full coverage available from **[e.g., last 12–24 months]**
- Earlier periods may be:
  - Aggregated
  - Sampled
  - Missing anonymous sessions

---

### 3.2.3 Returns History

- Often incomplete in early periods
- Return reasons may be missing or unstandardized
- Lag between order and return can exceed feature windows

---

### 3.2.4 Customer Metadata History

- Signup dates reliable from system inception
- Historical changes (e.g., loyalty tier) may not be fully tracked
- Backfilled attributes may overwrite historical truth

---

## 3.3 Data Reliability Assumptions

This section documents **explicit assumptions** the system relies on.

If these assumptions break, **model outputs may become invalid**.

---

### 3.3.1 Timestamp Assumptions

- All timestamps are assumed to be:
  - In a consistent timezone OR correctly normalized
  - Monotonically increasing per entity
- Event time ≠ ingestion time
- Late-arriving events are expected and handled

**Risk**  
Incorrect timestamps can cause:
- Label leakage
- Incorrect recency features
- Invalid churn labels

---

### 3.3.2 Customer Identity Assumptions

- `customer_id` is stable across systems
- Session-to-customer stitching is:
  - Partial
  - Probabilistic in early lifecycle
- Anonymous activity may be under-attributed

**Risk**  
Identity fragmentation leads to:
- Underestimated engagement
- Inflated churn risk for active users

---

### 3.3.3 Missing Data Assumptions

- Missing values are **not random**
- Absence of events may mean:
  - True inactivity
  - Tracking failure
- Null monetary values do not imply zero spend

**Handling Strategy**
- Explicit missing indicators
- Window-based defaults
- Conservative imputation

---

### 3.3.4 Schema Stability Assumptions

- Core fields (`customer_id`, timestamps, amounts) are stable
- New columns may be added without notice
- Enumerations (event types, return reasons) may evolve

**Risk**  
Silent schema changes can:
- Break feature pipelines
- Shift feature distributions

---

## 3.4 Known Biases & Gaps

No dataset is neutral.  
This system explicitly acknowledges known biases.

---

### 3.4.1 Channel Bias

- Offline or assisted sales may be underrepresented
- Mobile vs web tracking fidelity differs
- Some channels generate richer event data

**Impact**
- Engagement-heavy segments may skew toward digital-native users

---

### 3.4.2 Customer Lifecycle Bias

- New customers have limited history
- Cold-start predictions rely heavily on population priors
- Early churn may be misclassified due to sparse data

---

### 3.4.3 Geographic & Temporal Bias

- Certain regions may have:
  - Shorter data history
  - Different return behavior
- Promotional periods distort normal behavior
- Pandemic or exceptional events skew trends

---

### 3.4.4 Product / SKU Bias

- High-frequency, low-value SKUs dominate event counts
- Long-tail products may be under-learned
- Returns behavior differs widely by category

---

### 3.4.5 Survivorship Bias

- Historical data overrepresents retained customers
- Churned customers disappear from later periods
- CLV models risk optimism bias

---

## 3.5 Data Quality Monitoring (Awareness Only)

This system assumes that **active monitoring exists** for:

- Data freshness
- Row counts and volume anomalies
- Feature distribution drift
- Missing value spikes

This section documents **assumptions**, not implementation.

---

## 3.6 Summary

The Customer Intelligence System is **only as reliable as its data**.

This section makes explicit:
- Where data comes from
- How complete it is
- What assumptions are made
- Where known blind spots exist

By documenting these upfront, the system avoids:
- Silent failure
- Overconfidence in predictions
- Misuse of outputs in high-stakes decisions